---
title: "Weekly Work"
format:
  html:
    code-fold: true
jupyter: python3
---

## Week 1

For the first week I focused on **understanding** the package provided and using it effectively. To test this I created a dataset, trained a model on it, tested the model then did some analysis.

To generate data, the program sampler.py is used and for the set generated here, a desnity of 0.1 is used. This creates by default the evenly indexed tumbling rates 0.016, 0.034, 0.073, 0.157 and 0.340 as shown in @fig-initialData. To generate the odd set requires another run of the program with the added --odd suffix added. A csv file is also created to keep track of the metadata of these files. The values used are chosen from a exponential scale meaning there is a preference to lower tumbling rates in the dataset. 

![Initial dataset](figures/week1/dataImage.png){#fig-initialData}

The neural network layers used by the previous authors is presumed to have been refine dover time for optimal results for their findings and can be left as is for now. It is found in the cp_network.py and cp_network_multi.py files. I used it to generate a model on firstly the even dataset then once the odd dataset had been generated as wellm, both of them for the second model. Due to the tumbling rates being generated logarithmically, the even dataset is generally made up of lower tumbling rates and when tested will have to interpolate the odd regions. As they can take a while to train and depend highly on the dat generated, their weights are saved for later use including here.

#### Layers Of Models Used

```{python}
import tensorflow as tf

name1 = "0.1_even"
name2 = "0.1_full"
model1 = tf.keras.models.load_model(f'../models/week1/{name1}.keras')
model2 = tf.keras.models.load_model(f'../models/week1/{name2}.keras')
model1.summary()

```

The models are then both tested on the whole dataset, rather naively the same one which despite a test train split may be overfitted giving falsely high accuracies. This was done through 10 epochs of the whole dataset taking roughly 45 seonds per epoch on my machine (M1 mac mini, no GPU access). I created the notebook dataset_comparison.ipynb to test two models and compare them graphically to see how they differ. Model 1 is trained on just the even indexed datsets, model2 on the entire dataset. The next three plots show the same data in different formats, so here I want to choose which one best represents the data at scale while maintaining a view opf the accuracy. @fig-week1Line effectively shows that model 1 has trained well to predicted data at low tumbling rates and interpolated well around that region but becomes less precise at higher values, whereas model 2 generalised better but this is to be expected as it was given more data. It also showed why the previous pair who worked on this chose to limit the tumbling rate to 0.5 as it seems the precision is lost aroudn this vlaue. However this could be just because there less data around here due to the logarithmic scale used.


![Violin plot of predicted tumbling rates vs. true tumbling rate. Blue model is trained on a dataset made up of the evenly indexed tumbling rates and the orange model from both the evenly and odd tumbling rates.](figures/week1/comparison1.png){#fig-week1Violin}

![Line plot of mean predicted tumbling rate vs. true tumbling rate. Blue model is trained on a dataset made up of the evenly indexed tumbling rates and the orange model from both the evenly and odd tumbling rates.](figures/week1/comparison2.png){#fig-week1Line}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates. Blue model is trained on a dataset made up of the evenly indexed tumbling rates and the orange model from both the evenly and odd tumbling rates.](figures/week1/comparison3.png){#fig-week1Scatter}


## Week 2

The plan going forward is to compare specifically two aprts of the model: extrapolation and dataset size. Last week shwoed that the model even given a relatively limited range of data can interpolate well, however we could generate data of lower tumbling rates and predict them with higher ones inn the test data and see how much accuracy is lost. This can easily be repeated for the opposite to see if the model is well optimised to extrapolate to lower tumbling rates too.

Model 1 is trained on the first half of the data (0.016. 0.023, 0.034, 0.05 & 0.073) and model 2 on the second half (0.107, 0.157, 0.231, 0.34, 0.5). They are both then tested on the whole data to provide the following graphs. @fig-week2Violin shows that spread of predictions for model 1 is significantly smaller than model 2 for all tumbling rates, even very high ones and excels as predicting vlaues on the test data for low tumbling rates but predictably loses this precision at higher ones. Model 2 still has a relatively low spread of predictions for low data and a very high spread even for data it is largely trained on. This could be due to the logarithmic scale of tumbling rates used meaning it actually is trying to fit over a larger range of values than model 1 so has less accuracy.

@fig-week2Line largely supports this and shows model 2 seems to have an optimal prediction actually outside the its training data at 0.073. 

![Violin plot of predicted tumbling rates vs. true tumbling rate. Blue model trained on first half of tumbling rates and orange trained on last half of tumbling rates data.](figures/week2/modelComparison1.png){#fig-week2Violin}

![Line plot of mean predicted tumbling rate vs. true tumbling rate. Blue model trained on first half of tumbling rates and orange trained on last half of tumbling rates data.](figures/week2/modelComparison2.png){#fig-week2Line}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates. Blue model trained on first half of tumbling rates and orange trained on last half of tumbling rates data.](figures/week2/modelComparison3.png){#fig-week2Scatter}

Before starting the next part of training models on differing amounts of data, its worth looking at the code for generating the datasets in sampler.py. The important parts here are (altered here but gives useful information):

```{python}

tumble = 0.5  #example tumbling rate
snapshot = int(1 / tumble)

iters = int(1000 * (1 / tumble))

for iteration in range(iters):
  # evolve lattice one time step, calls a c file file function
  if (iteration % snapshot) != 0:
      continue

```

So it will output 1000 snapshots no matter the tumbling rate. However, as mentioned in their report, the characteristic evolution of a particle goes with the inverse of it's tumbling rate (on average a particle with tumbling probability 0.1 will move in the same direction for 10 time steps). Hence this method allows the system to evolve to a similar level and only output 1000 time frames despite varying length of simulation. Here we generate datasets with a desnity of 0.1 with 250, 500, 750, 1000 and 1250 time steps and compae their performances. Training the models is typically a long process as is testing.

```{python}
import pandas as pd

colNames = ["Model Number", "Number of Snapshots", "Time to Train (s)"]

data  = [["1", "250", "75"], ["2", "500","204"], ["3", "750", "314"], ["4", "1000","420"], ["5", "1250","555"]]

df = pd.DataFrame(columns=colNames, data=data)
df = df.set_index("Model Number")


df.head()

```

Here are the resulting graphs.

::: {#fig-datasetSizeComparison layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates. Models trained on all tumbling rates at varying numbers of snapshots output for training.](figures/week2/modelsComparisonViolin.png){#fig-week2ViolinMulti}

![Line plot of mean predicted tumbling rate vs. true tumbling rates. Models trained on all tumbling rates at varying numbers of snapshots output for training.](figures/week2/modelsComparisonLine.png){#fig-week2LineMulti}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates. Models trained on all tumbling rates at varying numbers of snapshots output for training.](figures/week2/modelsComparisonScatter.png){#fig-week2ScatterMulti}

Plots comparing 5 models trained on 5 dataset sizes, model 1 being 250 snapshots, 2 500, 3 750, 4 1000 and 5 1250.
:::


The main important things to mention here are model4 uses the 1000 snapshots used so far until this run, which is the cyan set. All models are then tested on the same dataset of 1000 snapshots, however this is a different run completely to the data used to train model4. @fig-week2LineMulti is the most useful graph here and shows an interesting result that the dataset with only 500 snapshots, model 2, has the highest precision compaed to accuracy for all tumbling rates (except for 0.05) until around 0.1 where it quickly becomes least accurate (except 0.34). This could be because the clustering that occurs in this se is similar to the testing set so the model is uniquely capable of predicting this data.

## Week 3

After the results of last week some further testing is taken. Firstly we can look at the kernels of the models to compare how they are visually and then if there are patterns that are found, they can be explored. The result that a model trained on less data can be more accurate than one with seemingly signifcantly more is unexpected so will be built on. For this we will try running a simulation with the same parameters multiple times to see if there is a significant difference between the models. The key reason is likely that the test data has patterns closest to the model with less data hence why it was a good predictor, so we can expand this by using different test data and see if the same model is still most accurate. Another things to test is to see if the model is still a good predictor for data its not trained on i.e. extrapolate again. Finally another thing we want to see is more extreme data to see if the model can be broken orbe made siginificantly better at predicting.

To start on kernel analysis is simple as we have the models saved. However when trying to see if I could change the size of the kernels to display more information than just a 3x3 grid, I found anothe rmodel using 5x5 kernels in the code. It seemed like it could have been used by the previus group possibly more so I re-trained on the 250 snapshots data. This took 228 seconds, compared to 75 seconds with the other model. The results can be seen in @fig-week3Violin250, @fig-week3Line250 and @fig-week3Scatter250. The lack in difference in results prompted me to do the same for 1250 snapshots, where the training took 555 seconds before, now takes close to three quarters of an hour. The results of testing are shown in but due to the time increase, the original model will be continued to be used predominantly.

```{python}

# Old Model

def make_net_1(shape):
    model = Sequential()

    model.add(Conv2D(filters=3, kernel_size=(3,3), padding='same', strides=(3,3), activation='relu', input_shape=shape))
    model.add(BatchNormalization())
    model.add(Conv2D(filters=3, kernel_size=(3,3), padding='same'))
    model.add(BatchNormalization())

    model.add(MaxPooling2D(pool_size=(3, 3)))

    #model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2)))

    model.add(Conv2D(filters=6, kernel_size=(3,3), padding='same'))
    model.add(BatchNormalization())

    model.add(Conv2D(filters=6, kernel_size=(3,3), padding='same'))
    model.add(BatchNormalization())

    model.add(MaxPooling2D(pool_size=(3, 3)))

    model.add(Dense(units=128, activation='relu'))

    with options({"layout_optimizer": False}):
        model.add(Dropout(0.2))
    model.add(Dense(units=10, activation='softmax'))

    model.add(Flatten())
    model.add(Dense(units=1, activation='linear'))
    return model

# New Model

def make_net_2(shape):
    model = Sequential()

    model.add(Conv2D(filters=3, kernel_size=(3, 3), padding="same", input_shape=shape))
    model.add(MaxPooling2D(pool_size=(2, 2), padding="same"))
    model.add(ReLU())
    model.add(BatchNormalization())

    model.add(Conv2D(filters=4, kernel_size=(5, 5), padding="same"))
    model.add(MaxPooling2D(pool_size=(2, 2), padding="same"))
    model.add(ReLU())
    model.add(BatchNormalization())

    model.add(Conv2D(filters=6, kernel_size=(5, 5), padding="same"))
    model.add(MaxPooling2D(pool_size=(2, 2), padding="same"))
    model.add(ReLU())
    model.add(BatchNormalization())

    model.add(GlobalAveragePooling2D())

    with options({"layout_optimizer": False}):
        model.add(Dropout(0.1))

    model.add(Dense(units=128, activation="relu"))

    with options({"layout_optimizer": False}):
        model.add(Dropout(0.1))

    model.add(Dense(units=3, activation="relu"))

    model.add(Flatten())
    model.add(Dense(units=1, activation="linear"))
    return model
```


::: {#fig-architectureComparison250 layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rate. Model 1 is old architecture, model 2 the new, slower one.](figures/week3/architectureComparisonViolin250.png){#fig-week3Violin250}

![Line plot of mean predicted tumbling rate vs. true tumbling rate. Model 1 is old architecture, model 2 the new, slower one.](figures/week3/architectureComparisonLine250.png){#fig-week3Line250}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates. Model 1 is old architecture, model 2 the new, slower one.](figures/week3/architectureComparisonScatter250.png){#fig-week3Scatter250}

Plots comparing two network architectures, model 1 with 3x3 kernels and model 2 with 5x5 kernels, also shown in code snippet above. Model 1 took 75 seconds and model 2 took 228 seconds to train.
:::

---

And now for the comparisons of the architecture for larger amounts of data.

::: {#fig-architectureComparison1250 layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates. Model 1 is old architecture, model 2 the new, slower one.](figures/week3/architectureComparisonViolin1250.png){#fig-week3Violin1250}

![Line plot of mean predicted tumbling rate vs. true tumbling rates. Model 1 is old architecture, model 2 the new, slower one.](figures/week3/architectureComparisonLine1250.png){#fig-week3Line1250}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates. Model 1 is old architecture, model 2 the new, slower one.](figures/week3/architectureComparisonScatter1250.png){#fig-week3Scatter1250}

Plots comparing two network architectures, model 1 with 3x3 kernels and model 2 with 5x5 kernels, also shown in code snippet above. Model 1 took 9 minutes and model 2 took 45 minutes to train.
:::

Now to compare the models with extrapolation I repeat the method for week 2 but in three overarching steps. One where I remove the first 3 tumbling rates from the data, one where I remove the last three tumbling rates from the data and one where i remove a middle three datasets (0.05, 0.073, 0.107). This is repeated for all datasets as well. This took a while to generate the models but the only other important information here is that from this point on the test dataset will now be used in it's entirety instead of just a split. This is because that data is not being used for training so the whole data can be used for validation. This makes testing take a while longer.

---

::: {#fig-datasetComparisonLower layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates.](figures/week3/dataComparisonLowerViolin.png){#fig-week3ViolinLower}

![Line plot of mean predicted tumbling rate vs. true tumbling rates.](figures/week3/dataComparisonLowerLine.png){#fig-week3LineLower}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates.](figures/week3/dataComparisonLowerScatter.png){#fig-week3ScatterLower}

Plots comparing how the size of a dataset affects the model, model 1 is trained on 250 snapshots, 2 on 500, 3 on 750, 4 on 1000 and 5 on 1250. The models here are all trained on a subset of the tumbling rates, where the largest 3 (0.231, 0.340 and 0.500) are not used in training.
:::

---

::: {#fig-datasetComparisonUpper layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates.](figures/week3/dataComparisonUpperViolin.png){#fig-week3ViolinUpper}

![Line plot of mean predicted tumbling rate vs. true tumbling rates.](figures/week3/dataComparisonUpperLine.png){#fig-week3LineUpper}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates.](figures/week3/dataComparisonUpperScatter.png){#fig-week3ScatterUpper}

Plots comparing how the size of a dataset affects the model, model 1 is trained on 250 snapshots, 2 on 500, 3 on 750, 4 on 1000 and 5 on 1250. The models here are all trained on a subset of the tumbling rates, where the lowest 3 (0.016, 0.023 and 0.034) are not used in training.
:::

---

::: {#fig-datasetComparisonOuter layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates.](figures/week3/dataComparisonOuterViolin.png){#fig-week3ViolinOuter}

![Line plot of mean predicted tumbling rate vs. true tumbling rates.](figures/week3/dataComparisonOuterLine.png){#fig-week3LineOuter}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates.](figures/week3/dataComparisonOuterScatter.png){#fig-week3ScatterOuter}

Plots comparing how the size of a dataset affects the model, model 1 is trained on 250 snapshots, 2 on 500, 3 on 750, 4 on 1000 and 5 on 1250. The models here are all trained on a subset of the tumbling rates, where the middle 3 (0.05, 0.073, 0.107) are not used in training.
:::

---

#### Repeated Models
Here I will run the same simulation of 1000 snapshots with density 0.1 as has been used throughout this study and train a model on each 4 of these repetitions. I can then compae their performances to compare to previous and future uses to use as a basis of how important the clustering of the simulation is or if the significance of chosen parameters is what gives rise to differences in model performance.

::: {#fig-repeatedModel layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates.](figures/week3/sameParametersViolin.png){#fig-week3RepeatedModelViolin}

![Line plot of mean predicted tumbling rate vs. true tumbling rates.](figures/week3/sameParametersLine.png){#fig-week3RepeatedModelOuter}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates.](figures/week3/sameParametersScatter.png){#fig-week3RepeatedModelOuter}

Plots comparing how 4 models based off 4 distinct simulations with the same parameters compare. Any variation can be attributed to how much the randomness of each simulation affects the prediction, including similarities to the test data.
:::


## Week 4

Firstly I had some work carried over from week 3. This was to take the kernels generated by the neural network and compare their shape to a set of filter matrices that are used in image processing to see if the machine learning had naturally replicated them. This is carried out in edge_filters.ipynb. A few changes are made but we can see how close each kernel is to each filter and sort them by this closeness with normalised magnitudes. Additionally, the kernel analysis will be moved across to another file as it is a distinct study compared to the model performance here.


We also looked back over the work I did from last week on the performance on the network on predictions and noticed something strange about particularly @fig-week3ViolinUpper. Here the dataset with the lowest amount of training data did the best prediction but fitted to its trained tumbling rates worst, whereas the dataset with the most training data struggled to predict to lower tumbling rates. Two possible ways of investigating this will be carried out: sample the tumbling rates linearly instead of logarithmically and repeat the same steps and the second method is to see if the model is overfitting by looking at the loss of the training and seeing if the number of epochs is limiting the training of the larger dataset.

Test loss comes from the following code:

```{python}
# print("Evaluate on test data:")
# results = model.evaluate(x_val, y_val, batch_size=64, verbose=0)
# print("Test loss:", results[0])
# print("Test accuracy:", results[1])
```

Reminder, upper means the training data includes the upper 7 tumbling rates, lower the lower 7 tumbling rates and outer the outer 7 tumbling rates.


```{python}
colNames = ["Name", "Test Loss", "Final Epoch Loss", "Data Split (test:train)", "Time (s)"]

data  = [["250 lower", "0.011396143585443497", "0.0140", "4500:20000", "40"], ["750 lower", "0.00855135265737772", "0.0109", "53500:0000", "210"], ["1250 lower", "0.007289157714694738", "0.0098", "102500:20000", "380"], ["250 upper", "0.14913895726203918", "0.0554", "4500:20000", "40"], ["750 upper", "0.06558762490749359", "0.0401", "53500:0000", "210"], ["1250 upper", "0.044244393706321716", "0.0433", "102500:20000", "380"]]

df = pd.DataFrame(columns=colNames, data=data)
df = df.set_index("Name")


df.head(6)

```


## Week 5


Clearly there is some difference between not only the different data sizes but also for the tumbling rates chosen. I'll get advice on what to do about this and then hopefully have better adjusted models. For now I can edit the original code that generated the datasets to output some linearlly selected tumbling rates. The belief is that because the changes in evolution happen on an exponential scale there will be big jumps and plateaus between selected tumbling rates.

Also just a note for myself, I still havent pushed the data sizes to extremes to break the model yet.

---

#### Linear tumbling rates

Firstly data was trained on tumbling rates between the previously used values: 0.016 and 0.5 with 10 equally spaced tumbling rates between them. In @fig-linearOnExponential it's first tested on the original exponentially spaced tumbling rates, then in @fig-linearOnLinear they are then tested on also linearlly spaced tumbling rates data.

::: {#fig-linearOnExponential layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates.](figures/week4/dataComparisonLinearOnExponentialViolin.png){#fig-week4LinearOnExponentialViolin}

![Line plot of mean predicted tumbling rate vs. true tumbling rates.](figures/week4/dataComparisonLinearOnExponentialLine.png){#fig-week4LinearOnExponentialLine}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates.](figures/week4/dataComparisonLinearOnExponentialScatter.png){#fig-week4LinearOnExponentialScatter}

Plots comparing how differently sized datasets from tumbling rates generated linearly rather than exponentially predict exponentially spaced test data.
:::

---

::: {#fig-linearOnLinear layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates.](figures/week4/dataComparisonLinearOnLinearViolin.png){#fig-week4LinearOnLinearViolin}

![Line plot of mean predicted tumbling rate vs. true tumbling rates.](figures/week4/dataComparisonLinearOnLinearLine.png){#fig-week4LinearOnLinearLine}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates.](figures/week4/dataComparisonLinearOnLinearScatter.png){#fig-week4LinearOnLinearScatter}

Plots comparing how differently sized datasets from tumbling rates generated linearly rather than exponentially predict also linearly spaced test data.
:::


---

Here we now compare both models, one trained on linearlly spaced tumbling rates, the other exponentially. One diagram will compare both tested on linearlly spaced tumbling rates, the other on exponentially spaced tumbling rates.

::: {#fig-linearOnLinear layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates. The models are both tested on linearlly spaced tumbling rates.](figures/week4/tumblingRatesComparisonLinearViolin.png){#fig-week4LinearSpacingTestViolin}

![Line plot of mean predicted tumbling rate vs. true tumbling rates. The models are both tested on linearlly spaced tumbling rates.](figures/week4/tumblingRatesComparisonLinearLine.png){#fig-week4LinearSpacingTestLine}

![Violin plot of predicted tumbling rates vs. true tumbling rates. The models are both tested on exponentially spaced tumbling rates.](figures/week4/tumblingRatesComparisonExponentialViolin.png){#fig-week4ExponentialSpacingTestViolin}

![Line plot of predicted tumbling rates vs. true tumbling rates. The models are both tested on exponentially spaced tumbling rates.](figures/week4/tumblingRatesComparisonExponentialLine.png){#fig-week4ExponentialSpacingTestLine}


Plots comparing how a model trained on linearlly spaced tumbling rates compares to a model trained on exponentially spaced tumbling rates. These are both tested on linearly spaced tumbling rates then exponentially spaced tumbling rates.
:::

---

Here I will train models on siginifacntly smaller and larger amounts of data: 50, 100, 150 and 200 time steps as well as 1500 and 1750 then comapre them to previous values

::: {#fig-dataExtremes layout-ncol=2}

![Line plot of predicted tumbling rates vs. true tumbling rates for high datasizes.](figures/week5/largeDatsizeComparisonLine.png){#fig-week5LargeDatasizeComparison}

![Violin plot of mean predicted tumbling rate vs. true tumbling rates for low datasizes.](figures/week5/smallDatsizeComparisonViolin.png){#fig-week5smallDatasizeComparisonViolin}

![Line plot of mean predicted tumbling rate vs. true tumbling rates for low datasizes.](figures/week5/smallDatsizeComparisonLine.png){#fig-week5smallDatasizeComparisonLine}

Plots comparing how models trained at different extremes perform. The ;arger datasets performed similarly to the other high datasets as expected however the smaller one sgave much less significant changes than expected so further testing will be performed. A possible reason for this is I gave too many epochs to achieve similar losses so the next two models will also share the same 20 epochs as used for 50 & 100 snapshots.
:::

::: {#fig-dataExtremesLower layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates for extra small datasizes.](figures/week5/extraSmallDatsizeComparisonViolin.png){#fig-week5extraSmallDatasizeComparisonViolin}

![Line plot of mean predicted tumbling rate vs. true tumbling rates for extra small datasizes.](figures/week5/extraSmallDatsizeComparisonLine.png){#fig-week5extraSmallDatasizeComparisonLine}

Plots comparing how models trained at extremely low data inputs perform. Here we have the expected result of the lowest training size having by far the worst performance. The value used is 10 which theoretically only output 10 snapshots which if thats the case shows that the model clearly doesnt need much data to get a good model (250 for example)|
:::

## Week 6

For the final week the plan is to go voer a couple things missed from the previous week, switch the autoencoder idea and write a summary of work done. Firstly, I haven't covered the week 5 work much here as not much progress was made in the desired direction with the autoencoder. The model used here shrinks a 128x128 image to a single value, so the idea of using the single value to then decode wont result in much information at the end result. To make up for this I tried removing layers from the encoder to retain more information and try to decode them which also didnt have much success, so in week 6 we will see what outputs of a reduced encoder would look like. Additionally with the shape of the input iamge and pooling of 3x3 grids, the upscaling doesnt allow to match the same size as data without further layers to not mirror the encoder.

First things done was to produce a learning curve image comparing how the different data sizes reduce their loss through evolving epochs. This was done to see if more epochs could be beneficial for some data sizes. Their results are shown in @fig-week6LearningCurves. The final loss isnt expected to be related to the size of the training data however it could be argued that 250 iterations should be given additional epochs to reach a lower loss.

---
#### Learning curves

![Learning curve of different datasizes with the same tumbling rates (logspace).](figures/week6/learningCurves.png){#fig-week6LearningCurves}

![Validation learning curve of different datasizes with the same tumbling rates (logspace).](figures/week6/rollingValidationLearningCurves.png){#fig-week6RollingValidationLearningCurves}

Next we want to follow up on last week's work by creating a smaller model and looking at how the different predictions work with different model complexities to try and understand the model more. For this reaso, it links too closely to kernal analysis to be included here and will be in the kernal analysis weekly work file.

Finally, one piece of information was realised far too late into this project and this was completely my fault for not seeing it for so long. Whenever I loaded data I saw large sizes beyond what i expected. For example 10 tumbling rates with 250 snapshots output should have resulted in 2500 output images, however gave 14 times this amount, which is because rolling is in the default code where the image shifted 14 times to see if the model can learn from this random shifting. This makes a stronger model by giving more data, making it learn that the position of clusters doesnt matter and also adding noise to the data.

Now I generate datasets for 250,500,750,1000 adn 1250 output frames without the rolling and train models the same way with only 10 epochs. Doing this gave the following epochs:

![Learning curve of different datasizes with the same tumbling rates (logspace). The data used to trian these has no rolling.](figures/week6/noRollingLearningCurves.png){#fig-week6NoRollingLearningCurves}

![Validation learning curve of different datasizes with the same tumbling rates (logspace). The data used to trian these has no rolling.](figures/week6/noRollingValidationLearningCurves.png){#fig-week6NoRollingValidationLearningCurves}

Its clear from this figure that the loss is larger than that of the rolling datasets and will take many more epochs to reach a similar loss. I will keep the mdoels trained with these losses. Below I compare a model trained on rolling data vs. on data without the rolls. 

---
#### No rolling trained model vs. rolling trained model

::: {#fig-rollingVsNoRolling layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates for a model trained on data with and without rolling. Tested on data with rolling.](figures/week6/rollingVsNoRollingTestedRollViolin.png){#fig-week6RollingVsNoRollingRollTestViolin}

![Line plot of predicted tumbling rates vs. true tumbling rates for a model trained on data with and without rolling. Tested on data with rolling.](figures/week6/rollingVsNoRollingTestedRollLine.png){#fig-week6RollingVsNoRollingRollTestLine}

![Violin plot of predicted tumbling rates vs. true tumbling rates for a model trained on data with and without rolling. Tested on data without rolling.](figures/week6/rollingVsNoRollingTestedNoRollViolin.png){#fig-week6RollingVsNoRollingNoRollTestViolin}

![Line plot of predicted tumbling rates vs. true tumbling rates for a model trained on data with and without rolling. Tested on data without rolling.](figures/week6/rollingVsNoRollingTestedNoRollLine.png){#fig-week6RollingVsNoRollingNoRollTestLine}

Plots comparing how models trained on data with rolling comapre to models trained on data without rolling. The same number of output frames are used but the rolling data has 14 extra rolls for each frame. On all figures it's clear that the model trained on data with the rolls performs better, especially at higher tumbling rates. Given the tumbles ar enot new information, just shifted copies of the original data, it's valid to say these are very helpful to generate.
:::

---
#### No rolling trained models datasize comparison

Now we can see how changing the size of samples taken affects the accuracy of this. These are tested on rolling data as no signigfcant difference was found between the two in @fig-rollingVsNoRolling but gives more data which may be helpful.

::: {#fig-noRollingComparisons layout-ncol=2}

![Violin plot of predicted tumbling rates vs. true tumbling rates for models trained on data without rolling of different sizes. Tested on data with rolling.](figures/week6/noRollComparisonsViolin.png){#fig-week6NoRollComparisonsViolin}

![Line plot of predicted tumbling rates vs. true tumbling rates for models trained on data without rolling of different sizes. Tested on data with rolling.](figures/week6/noRollComparisonsLine.png){#fig-week6NoRollComparisonsLine}

![Scatter plot of predicted tumbling rates vs. true tumbling rates for models trained on data without rolling of different sizes. Tested on data with rolling.](figures/week6/noRollComparisonsScatter.png){#fig-week6NoRollComparisonsScatter}


Plots comparing how models trained on data without rolling of different sizes can predict test data with rolling. The violin plot is especially very 'ideal' where the datasizes are clearly separated with distinct collections, some having no overlap at all.
:::

#### Batch size and learning rate

As the project comes to a close theres a few things to note that were kept the same throughout. Namely the model was not changed as well as the following pieces of code for trainign a model:

```{python}
# from tensorflow import keras

# optimizer = keras.optimizers.Adam(learning_rate=0.001)
# model.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['accuracy'])

 
# history = model.fit(
#     x_train,
#     y_train,
#     epochs=10,
#     verbose=True,
#     batch_size=64,
#     validation_data=(x_val, y_val)
# )

```

So the optimiser was always from a set one called Adam with learning rate 0.001, the loss was always mean absolute error, epochs always 10 (except for a few times I mentioned I changed it) and a batch size of 64, always. I used these as they were given by the previous two project members and not changing them gave the most fair comparisons from one test to the other. However if you wanted to find the most accurate network possible for the model then changing them is necessary.
