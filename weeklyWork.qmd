---
title: "Weekly Work"
format:
  html:
    code-fold: true
jupyter: python3
---

## Week 1

For the first week I focused on **understanding** the package provided and using it effectively. To test this I created a dataset, trained a model on it, tested the model then did some analysis.

To generate data, the program sampler.py is used and for the set generated here, a desnity of 0.1 is used. This creates by default the evenly indexed tumbling rates 0.016, 0.034, 0.073, 0.157 and 0.340 as shown in @fig-initialData. To generate the odd set requires another run of the program with the added --odd suffix added. A csv file is also created to keep track of the metadata of these files. The values used are chosen from a exponential scale meaning there is a preference to lower tumbling rates in the dataset. 

![Initial dataset](figures/week1/dataImage.png){#fig-initialData}

The neural network layers used by the previous authors is presumed to have been refine dover time for optimal results for their findings and can be left as is for now. It is found in the cp_network.py and cp_network_multi.py files. I used it to generate a model on firstly the even dataset then once the odd dataset had been generated as wellm, both of them for the second model. Due to the tumbling rates being generated logarithmically, the even dataset is generally made up of lower tumbling rates and when tested will have to interpolate the odd regions. As they can take a while to train and depend highly on the dat generated, their weights are saved for later use including here.

#### Layers Of Models Used

```{python}
import tensorflow as tf

name1 = "0.1_even"
name2 = "0.1_full"
model1 = tf.keras.models.load_model(f'../models/{name1}.keras')
model2 = tf.keras.models.load_model(f'../models/{name2}.keras')
model1.summary()

```

The models are then both tested on the whole dataset, rather naively the same one which despite a test train split may be overfitted giving falsely high accuracies. This was done through 10 epochs of the whole dataset taking roughly 45 seonds per epoch on my machine (M1 mac mini, no GPU access). I created the notebook dataset_comparison.ipynb to test two models and compare them graphically to see how they differ. Model 1 is trained on just the even indexed datsets, model2 on the entire dataset. The next three plots show the same data in different formats, so here I want to choose which one best represents the data at scale while maintaining a view opf the accuracy. @fig-week1Line effectively shows that model 1 has trained well to predicted data at low tumbling rates and interpolated well around that region but becomes less precise at higher values, whereas model 2 generalised better but this is to be expected as it was given more data. It also showed why the previous pair who worked on this chose to limit the tumbling rate to 0.5 as it seems the precision is lost aroudn this vlaue. However this could be just because there less data around here due to the logarithmic scale used.


![Violin plot of precited tumbling rates vs. true tumbling rate.](figures/week1/comparison1.png){#fig-week1Violin}

![Line plot of mean predicted tumbling rate vs. true tumbling rate.](figures/week1/comparison2.png){#fig-week1Line}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates.](figures/week1/comparison3.png){#fig-week1Scatter}


## Week 2

The plan going forward is to compare specifically two aprts of the model: extrapolation and dataset size. Last week shwoed that the model even given a relatively limited range of data can interpolate well, however we could generate data of lower tumbling rates and predict them with higher ones inn the test data and see how much accuracy is lost. This can easily be repeated for the opposite to see if the model is well optimised to extrapolate to lower tumbling rates too.

Model 1 is trained on the first half of the data (0.016. 0.023, 0.034, 0.05 & 0.073) and model 2 on the second half (0.107, 0.157, 0.231, 0.34, 0.5). They are both then tested on the whole data to provide the following graphs. @fig-week2Violin shows that spread of predictions for model 1 is significantly smaller than model 2 for all tumbling rates, even very high ones and excels as predicting vlaues on the test data for low tumbling rates but predictably loses this precision at higher ones. Model 2 still has a relatively low spread of predictions for low data and a very high spread even for data it is largely trained on. This could be due to the logarithmic scale of tumbling rates used meaning it actually is trying to fit over a larger range of values than model 1 so has less accuracy.

@fig-week2Line largely supports this andshows model 2 seems to have an optimal prediction actually outside the its training data at 0.073. 

![Violin plot of precited tumbling rates vs. true tumbling rate.](figures/week2/modelComaprison1.png){#fig-week2Violin}

![Line plot of mean predicted tumbling rate vs. true tumbling rate.](figures/week2/modelComaprison2.png){#fig-week2Line}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates.](figures/week2/modelComaprison3.png){#fig-week2Scatter}

Before starting the next part of training models on differing amounts of data, its worth looking at the code for generating the datasets in sampler.py. The important parts here are (altered here but gives useful information):

```{python}

tumble = 0.5  #example tumbling rate
snapshot = int(1 / tumble)

iters = int(1000 * (1 / tumble))

for iteration in range(iters):
  # evolve lattice one time step, calls a c file file function
  if (iteration % snapshot) != 0:
      continue

```

So it will output 1000 snapshots no matter the tumbling rate. However, as mentioned in their report, the characteristic evolution of a particle goes with the inverse of it's tumbling rate (on average a particle with tumbling probability 0.1 will move in the same direction for 10 time steps). Hence this method allows the system to evolve to a similar level and only output 1000 time frames despite varying length of simulation. Here we generate datasets with a desnity of 0.1 with 250, 500, 750, 1000 and 1250 time steps and comapre their performances. Training the models is typically a long process as is testing.

```{python}
import pandas as pd

colNames = ["Model Number", "Number of Snapshots", "Time to Train (s)"]

data  = [["1", "250", "75"], ["2", "500","204"], ["3", "750", "314"], ["4", "1000","~400"], ["5", "1250","555"]]

df = pd.DataFrame(columns=colNames, data=data)
df = df.set_index("Model Number")


df.head()

```

Here are the resulting graphs.

![Violin plot of precited tumbling rates vs. true tumbling rate for 5 dataset sizes.](figures/week2/modelsComaprisonViolin.png){#fig-week2ViolinMulti}

![Line plot of mean predicted tumbling rate vs. true tumbling rate for 5 dataset sizes.](figures/week2/modelsComaprisonLine.png){#fig-week2LineMulti}

![Scatter plot of all predicted tumbling rates vs. true tumbling rates for 5 dataset sizes.](figures/week2/modelsComaprisonScatter.png){#fig-week2ScatterMulti}

The main important things to mention here are model4 uses the 1000 snapshots used so far until this run, which is the cyan set. All models are then tested on the same dataset of 1000 snapshots, however this is a different run completely to the data used to train model4. @fig-week2LineMulti is the most useful graph here and shows an interesting result that the dataset with only 500 snapshots, model 2, has the highest precision comapred to accuracy for all tumbling rates (except for 0.05) until around 0.1 where it quickly becomes least accurate (except 0.34). This could be because the clustering that occurs in this se is similar to the testing set so the model is uniquely capable of predicting this data.